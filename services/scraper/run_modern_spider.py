#!/usr/bin/env python3
"""
Run Modern Edeka24 Spider

Script para ejecutar el spider moderno de Edeka24 con lÃ­mites de desarrollo.
Configurado para scrapear 5 productos desde el sitemap.
"""
import os
import sys
from pathlib import Path

# Add the project root to the Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings

# Change to the scraper directory
os.chdir(Path(__file__).parent)

def run_edeka24_spider():
    """Ejecuta el spider de Edeka24 con configuraciÃ³n de desarrollo."""
    
    print("ğŸª Running Edeka24 Modern Spider")
    print("=" * 50)
    print("ğŸ“ Mode: Development (5 products from sitemap)")
    print("ğŸŒ Source: https://www.edeka24.de/sitemaps/sitemap_0-products-0.xml")
    print("=" * 50)
    
    # Set working directory to modern_scraper
    os.chdir(Path(__file__).parent / "modern_scraper")
    
    # Get settings for development environment
    os.environ['SCRAPY_SETTINGS_MODULE'] = 'modern_scraper.settings.development'
    settings = get_project_settings()
    
    print(f"ğŸ“Š Environment: {settings.get('ENVIRONMENT', 'unknown')}")
    print(f"ğŸ•·ï¸  Bot name: {settings.get('BOT_NAME')}")
    print(f"ğŸ“¦ Item limit: {settings.get('CLOSESPIDER_ITEMCOUNT')}")
    print(f"â±ï¸  Download delay: {settings.get('DOWNLOAD_DELAY')}s")
    print(f"ğŸ”„ Concurrent requests: {settings.get('CONCURRENT_REQUESTS')}")
    print()
    
    # Create crawler process
    process = CrawlerProcess(settings)
    
    print("ğŸš€ Starting Edeka24 spider...")
    process.crawl('edeka24_spider')
    
    # Start the crawling process
    print("ğŸ Starting crawl process...")
    process.start()
    
    print("âœ… Edeka24 spider completed!")
    print()
    print("ğŸ“ Check the following files for results:")
    print("  - data/dev_products_*.jsonl.gz (scraped products)")
    print("  - debug/debug_*.log (debug information)")
    print("  - logs/dev_scraper.log (spider logs)")
    print()
    print("ğŸ” Results summary:")
    
    # Try to show a quick summary of results
    try:
        import glob
        import gzip
        import json
        
        # Find the most recent data file
        data_files = glob.glob("data/dev_products_*.jsonl.gz")
        if data_files:
            latest_file = max(data_files, key=os.path.getctime)
            print(f"ğŸ“„ Latest output: {latest_file}")
            
            # Count products
            product_count = 0
            with gzip.open(latest_file, 'rt', encoding='utf-8') as f:
                for line in f:
                    data = json.loads(line)
                    if 'name' in data and '_metadata' not in data and '_final_metadata' not in data:
                        product_count += 1
            
            print(f"ğŸ›’ Products scraped: {product_count}")
        else:
            print("âŒ No output files found")
            
    except Exception as e:
        print(f"âš ï¸  Could not read results: {e}")


if __name__ == "__main__":
    try:
        run_edeka24_spider()
    except KeyboardInterrupt:
        print("\nâ›” Spider interrupted by user")
    except Exception as e:
        print(f"\nâŒ Spider failed: {e}")
        import traceback
        traceback.print_exc()
